==============================
Emotion CNN Activation Maps Visualizer
==============================

Project Purpose
---------------
This project is designed to visualize the internal workings of a Convolutional Neural Network (CNN) trained for facial emotion recognition. It allows users to see which parts of a face image are most important for the model's classification decision by displaying activation maps and class activation maps (CAMs) for different layers of the network. The project is intended for educational, research, and explainable AI (XAI) purposes.

Project Architecture & Workflow
------------------------------
1. **Data Preparation**
   - The dataset consists of face images organized into folders by emotion (e.g., angry, happy, sad, etc.).
   - Each folder contains images for a specific emotion class.

2. **Model Training**
   - The model is based on a pre-trained ResNet18 architecture from PyTorch's torchvision library.
   - The final fully connected layer is replaced to match the number of emotion classes (typically 7).
   - Training is performed using the script `train_emotion_model.py`, which supports data augmentation and validation.
   - The model is trained using cross-entropy loss and the Adam optimizer.
   - Training and validation accuracy/loss are tracked and plotted.

3. **Activation Map Visualization**
   - The core logic is in `visualize_activations.py`.
   - The model is instrumented with forward hooks to capture activations from convolutional layers.
   - For a given input image, the model's activations are visualized as heatmaps, showing which regions of the image activate specific filters.
   - Class Activation Maps (CAMs) are generated by averaging the last convolutional layer's activations.

4. **User Interface (UI)**
   - The main UI is a Streamlit web app (`streamlit_app.py`).
   - Users can upload their own images or use a demo image.
   - The sidebar allows users to select the model, number of filters, and which layers to visualize.
   - The app displays the predicted emotion, CAM overlays, and filter-wise activation maps.
   - Users can download overlay images for further analysis.


Technologies, Tools, and Libraries
----------------------------------

**Python**
- The primary programming language for all scripts and the web app.

**PyTorch** (`torch`, `torchvision`)
- Used for model definition, training, and inference.
- Provides the ResNet18 architecture and pre-trained weights.
- Handles tensor operations, GPU acceleration, and autograd for training.

**NumPy**
- Used for numerical operations and array manipulation, especially for image processing and activation map handling.

**Pillow (PIL)**
- Used for image loading, saving, and conversion between formats.

**OpenCV (`opencv-python`)**
- Used for resizing images, applying color maps, and overlaying heatmaps on original images.

**Matplotlib**
- Used for plotting training history and displaying activation maps in scripts and notebooks.

**Streamlit**
- Provides the interactive web UI for uploading images, displaying results, and controlling visualization settings.
- Handles file uploads, sidebar controls, and dynamic image display.

**tqdm**
- Used for progress bars during model training.

**scikit-learn**
- (Optional) Can be used for additional metrics, confusion matrices, or data splitting.

**Jupyter**
- Used for interactive experimentation and demonstration in notebooks.

**ipywidgets**
- Used for interactive controls in Jupyter notebooks.

**Other Tools**
- **requirements.txt**: Lists all dependencies for easy installation.
- **README.md**: Provides user-facing documentation and usage instructions.


Key Files and Their Roles
-------------------------
- `train_emotion_model.py`: Handles model training, data loading, augmentation, and saving.
- `visualize_activations.py`: Contains the model definition and all logic for extracting and visualizing activation maps and CAMs.
- `streamlit_app.py`: Implements the web UI for interactive visualization.
- `requirements.txt`: Lists all required Python libraries.
- `README.md`: User and developer documentation.
- `outputs/`: Stores generated activation map images and overlays.
- `model/`: Stores trained model weights.
- `train/`, `test/`: Contain the dataset, organized by emotion class.


How It Works (End-to-End)
-------------------------
1. **Train the Model**
   - Run `train_emotion_model.py` to train a ResNet18 model on your dataset.
   - The script saves the best model weights to the `model/` directory.

2. **Run the Streamlit App**
   - Start the app with `streamlit run streamlit_app.py`.
   - Upload an image or use the demo image.
   - The app loads the model, preprocesses the image, and runs inference.
   - Activation maps and CAM overlays are generated and displayed.
   - Users can interactively explore different layers and filters.

3. **Explore and Explain**
   - The app helps users understand which parts of the face the model focuses on for emotion recognition.
   - Useful for debugging, education, and explainable AI research.


Best Practices and Recommendations
----------------------------------
- Use as much data as possible for training; more data improves model generalization.
- Use data augmentation to increase dataset diversity.
- Monitor both training and validation accuracy/loss to avoid overfitting.
- Keep the order of emotion labels consistent between training and inference.
- Use a GPU for faster training if available.
- Regularly update dependencies for security and performance.


Contact & Contribution
----------------------
- Contributions are welcome! See the README for guidelines.
- For questions or issues, open an issue on the project repository. 